Reflection after today’s gitleaks/semgrep run

- **Unexpected hurdles**: `semgrep --config auto` first failed because the Mirage CA store had no trust anchors and then, even after pointing `SSL_CERT_FILE` to `/etc/ssl/cert.pem`, it tried to write `/Users/matthew.heard/.semgrep/semgrep.log` (outside our permission scope) and required network access to download `https://semgrep.dev/c/auto`. I sidestepped the log path by setting `XDG_CONFIG_HOME` inside the repo, but the restricted network still prevents fetching the remote rules, so the CLI can’t finish. We may need to use a locally bundled rule set next time.
- **Gitleaks findings**: The scan reported 954 leaks because the Sonar JSON dumps (root `sonar_issues.json`, `reports/sonar/sonar_issues.json`, and `reports/sonar/issues.json`) contain generic API keys and the helper scripts expose inline curl creds. I created beads `dadeto-0ql`, `dadeto-55v`, and `dadeto-y54` that describe how to sanitize or parameterize those assets without copying any secrets, then deleted the `gitleaks-report.json` output to keep the workspace clean. The sonar snapshots may need to be generated on demand rather than stored in git so secrets don’t leak again.
- **Next idea / open question**: Which secrets/keys actually need rotation once the files are scrubbed, and should the sonar exports be removed entirely from version control so future scans stop tripping gitleaks? Documenting the desired workflow for regenerating those json blobs and securely consuming Sonar tokens would keep future agents from repeating this round of findings.
